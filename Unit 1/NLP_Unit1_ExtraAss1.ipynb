{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHr0doi1vWL2"
      },
      "outputs": [],
      "source": [
        "# Lab Assignment 1: Text Preprocessing and Regular Expressions\n",
        "# •\tImplement tokenization, stemming, and lemmatization using NLTK and spaCy.\n",
        "# •\tUse regular expressions for tasks such as extracting email addresses, phone numbers, and hashtags from a given text dataset of minimum 5 pages."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk spacy\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jm79pYSeBQbO",
        "outputId": "1f8276fb-6147-40e5-da68-4171bce602b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.2)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
            "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.1)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import spacy\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from collections import Counter\n",
        "\n",
        "# Download required NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Load SpaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Sample text (use a multi-page text document for real case)\n",
        "text = \"\"\"\n",
        "Contact us at info@example.com or support@company.org.\n",
        "You can also call +1-800-555-0199 or message us on Twitter #SupportTeam.\n",
        "Dr. John Smith joined Stanford University in 2021 and works in AI research.\n",
        "\"\"\"\n",
        "\n",
        "### 1. Tokenization (NLTK)\n",
        "print(\"\\n=== Tokenization (NLTK) ===\")\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)\n",
        "\n",
        "### 2. Stemming (NLTK)\n",
        "stemmer = PorterStemmer()\n",
        "stemmed = [stemmer.stem(word) for word in tokens]\n",
        "print(\"\\n=== Stemming (NLTK) ===\")\n",
        "print(stemmed)\n",
        "\n",
        "### 3. Lemmatization (NLTK & spaCy)\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmas_nltk = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "doc = nlp(text)\n",
        "lemmas_spacy = [token.lemma_ for token in doc]\n",
        "\n",
        "print(\"\\n=== Lemmatization (NLTK) ===\")\n",
        "print(lemmas_nltk)\n",
        "print(\"\\n=== Lemmatization (spaCy) ===\")\n",
        "print(lemmas_spacy)\n",
        "\n",
        "### 4. Regex: Extract emails, phone numbers, hashtags\n",
        "emails = re.findall(r'\\b[\\w.-]+?@\\w+?\\.\\w+?\\b', text)\n",
        "phones = re.findall(r'\\+?\\d[\\d\\-]{8,}\\d', text)\n",
        "hashtags = re.findall(r'#\\w+', text)\n",
        "\n",
        "print(\"\\n=== Extracted Emails ===\")\n",
        "print(emails)\n",
        "\n",
        "print(\"\\n=== Extracted Phone Numbers ===\")\n",
        "print(phones)\n",
        "\n",
        "print(\"\\n=== Extracted Hashtags ===\")\n",
        "print(hashtags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9k04P-WBsH5",
        "outputId": "a356ee27-2e98-419f-ff95-7e7bc0b1b762"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Tokenization (NLTK) ===\n",
            "['Contact', 'us', 'at', 'info', '@', 'example.com', 'or', 'support', '@', 'company.org', '.', 'You', 'can', 'also', 'call', '+1-800-555-0199', 'or', 'message', 'us', 'on', 'Twitter', '#', 'SupportTeam', '.', 'Dr.', 'John', 'Smith', 'joined', 'Stanford', 'University', 'in', '2021', 'and', 'works', 'in', 'AI', 'research', '.']\n",
            "\n",
            "=== Stemming (NLTK) ===\n",
            "['contact', 'us', 'at', 'info', '@', 'example.com', 'or', 'support', '@', 'company.org', '.', 'you', 'can', 'also', 'call', '+1-800-555-0199', 'or', 'messag', 'us', 'on', 'twitter', '#', 'supportteam', '.', 'dr.', 'john', 'smith', 'join', 'stanford', 'univers', 'in', '2021', 'and', 'work', 'in', 'ai', 'research', '.']\n",
            "\n",
            "=== Lemmatization (NLTK) ===\n",
            "['Contact', 'u', 'at', 'info', '@', 'example.com', 'or', 'support', '@', 'company.org', '.', 'You', 'can', 'also', 'call', '+1-800-555-0199', 'or', 'message', 'u', 'on', 'Twitter', '#', 'SupportTeam', '.', 'Dr.', 'John', 'Smith', 'joined', 'Stanford', 'University', 'in', '2021', 'and', 'work', 'in', 'AI', 'research', '.']\n",
            "\n",
            "=== Lemmatization (spaCy) ===\n",
            "['\\n', 'contact', 'we', 'at', 'info@example.com', 'or', 'support@company.org', '.', '\\n', 'you', 'can', 'also', 'call', '+1', '-', '800', '-', '555', '-', '0199', 'or', 'message', 'we', 'on', 'Twitter', '#', 'SupportTeam', '.', '\\n', 'Dr.', 'John', 'Smith', 'join', 'Stanford', 'University', 'in', '2021', 'and', 'work', 'in', 'AI', 'research', '.', '\\n']\n",
            "\n",
            "=== Extracted Emails ===\n",
            "['info@example.com', 'support@company.org']\n",
            "\n",
            "=== Extracted Phone Numbers ===\n",
            "['+1-800-555-0199']\n",
            "\n",
            "=== Extracted Hashtags ===\n",
            "['#SupportTeam']\n"
          ]
        }
      ]
    }
  ]
}